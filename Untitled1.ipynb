{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603abf9a-0b0e-4d02-a9cf-26a1b64d1c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/spidey/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spidey/miniconda3/envs/sus/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/spidey/miniconda3/envs/sus/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:01<00:00, 25.3MB/s]\n",
      "/tmp/ipykernel_5938/845803128.py:146: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [1/20] Train Loss: 0.8243, Train Acc: 0.7380 | Val Loss: 0.3748, Val Acc: 0.9076\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [2/20] Train Loss: 0.3494, Train Acc: 0.8881 | Val Loss: 0.2810, Val Acc: 0.9139\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [3/20] Train Loss: 0.2572, Train Acc: 0.9225 | Val Loss: 0.2555, Val Acc: 0.9162\n",
      "Epoch [4/20] Train Loss: 0.2226, Train Acc: 0.9268 | Val Loss: 0.2874, Val Acc: 0.9059\n",
      "Epoch [5/20] Train Loss: 0.2155, Train Acc: 0.9325 | Val Loss: 0.3148, Val Acc: 0.9007\n",
      "Epoch [6/20] Train Loss: 0.1778, Train Acc: 0.9401 | Val Loss: 0.2952, Val Acc: 0.8949\n",
      "Epoch [7/20] Train Loss: 0.1817, Train Acc: 0.9387 | Val Loss: 0.2834, Val Acc: 0.9013\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [8/20] Train Loss: 0.1528, Train Acc: 0.9498 | Val Loss: 0.2286, Val Acc: 0.9173\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [9/20] Train Loss: 0.1408, Train Acc: 0.9551 | Val Loss: 0.2365, Val Acc: 0.9191\n",
      "Epoch [10/20] Train Loss: 0.1340, Train Acc: 0.9568 | Val Loss: 0.2475, Val Acc: 0.9162\n",
      "Epoch [11/20] Train Loss: 0.1401, Train Acc: 0.9513 | Val Loss: 0.2617, Val Acc: 0.9093\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [12/20] Train Loss: 0.1468, Train Acc: 0.9491 | Val Loss: 0.2363, Val Acc: 0.9208\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [13/20] Train Loss: 0.1368, Train Acc: 0.9571 | Val Loss: 0.2145, Val Acc: 0.9242\n",
      "Epoch [14/20] Train Loss: 0.1392, Train Acc: 0.9516 | Val Loss: 0.2443, Val Acc: 0.9173\n",
      "Epoch [15/20] Train Loss: 0.1300, Train Acc: 0.9579 | Val Loss: 0.2369, Val Acc: 0.9168\n",
      "Epoch [16/20] Train Loss: 0.1340, Train Acc: 0.9576 | Val Loss: 0.2203, Val Acc: 0.9202\n",
      "Epoch [17/20] Train Loss: 0.1337, Train Acc: 0.9569 | Val Loss: 0.2378, Val Acc: 0.9179\n",
      "Epoch [18/20] Train Loss: 0.1337, Train Acc: 0.9571 | Val Loss: 0.2294, Val Acc: 0.9196\n",
      "Epoch [19/20] Train Loss: 0.1327, Train Acc: 0.9559 | Val Loss: 0.2268, Val Acc: 0.9214\n",
      "âœ… Best model saved to SavedModels/RoseResNet18_cuda.*\n",
      "Epoch [20/20] Train Loss: 0.1364, Train Acc: 0.9533 | Val Loss: 0.2119, Val Acc: 0.9277\n",
      "ðŸŽ¯ Final Best Validation Accuracy: 0.9277\n",
      "ðŸ“‚ All files saved inside: SavedModels/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ==============================\n",
    "# Configurations\n",
    "# ==============================\n",
    "train_dir = 'rose-3/train'\n",
    "validation_dir = 'rose-3/valid'\n",
    "test_dir = 'rose-3/test'\n",
    "\n",
    "batch_size = 32\n",
    "img_height, img_width = 224, 224   # ResNet input size\n",
    "epochs = 20\n",
    "model_name = \"RoseResNet18\"\n",
    "\n",
    "# Folder to save everything\n",
    "save_dir = \"SavedModels\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# ==============================\n",
    "# Data Preparation with Stronger Augmentations\n",
    "# ==============================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAffine(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(validation_dir, transform=val_transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# ==============================\n",
    "# Transfer Learning Model (ResNet18)\n",
    "# ==============================\n",
    "cnn_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze earlier layers\n",
    "for param in cnn_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final fully connected layer\n",
    "cnn_model.fc = nn.Sequential(\n",
    "    nn.Linear(cnn_model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "cnn_model = cnn_model.to(device)\n",
    "\n",
    "# ==============================\n",
    "# Training Setup\n",
    "# ==============================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.fc.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "# ==============================\n",
    "# Training Loop\n",
    "# ==============================\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    cnn_model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    cnn_model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = cnn_model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Save best model in all formats\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        save_name = f\"{model_name}_{device.type}\"\n",
    "\n",
    "        torch.save(cnn_model.state_dict(), os.path.join(save_dir, f\"{save_name}.pth\"))\n",
    "\n",
    "        scripted_model = torch.jit.script(cnn_model)\n",
    "        scripted_model.save(os.path.join(save_dir, f\"{save_name}.pt\"))\n",
    "\n",
    "        dummy_input = torch.randn(1, 3, img_height, img_width, device=device)\n",
    "        torch.onnx.export(\n",
    "            cnn_model, dummy_input, os.path.join(save_dir, f\"{save_name}.onnx\"),\n",
    "            input_names=[\"input\"], output_names=[\"output\"], \n",
    "            dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Best model saved to {save_dir}/{save_name}.*\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# Save Metrics and Plots\n",
    "# ==============================\n",
    "np.save(os.path.join(save_dir, \"train_losses.npy\"), np.array(train_losses))\n",
    "np.save(os.path.join(save_dir, \"val_losses.npy\"), np.array(val_losses))\n",
    "np.save(os.path.join(save_dir, \"train_accs.npy\"), np.array(train_accs))\n",
    "np.save(os.path.join(save_dir, \"val_accs.npy\"), np.array(val_accs))\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.legend(); plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\")\n",
    "plt.savefig(os.path.join(save_dir, \"loss_curve.png\")); plt.close()\n",
    "\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Validation Acc\")\n",
    "plt.legend(); plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(os.path.join(save_dir, \"accuracy_curve.png\")); plt.close()\n",
    "\n",
    "print(f\"ðŸŽ¯ Final Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"ðŸ“‚ All files saved inside: {save_dir}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44972d73-9275-4b73-8ad5-5a4d505f8194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
